{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 电影文本情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github地址](https://github.com/lijingpeng/kaggle/tree/master/competitions/Bag_of_Words)  \n",
    "[Kaggle地址](https://www.kaggle.com/c/word2vec-nlp-tutorial/)\n",
    "\n",
    "这个任务主要是对电影评论文本进行情感分类，主要分为正面评论和负面评论，所以是一个二分类问题，二分类模型我们可以选取一些常见的模型比如贝叶斯、逻辑回归等，这里挑战之一是文本内容的向量化，因此，我们首先尝试基于TF-IDF的向量化方法，然后尝试word2vec。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "    '''\n",
    "    # 去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 小写化所有的词，并转成词list\n",
    "    words = review_text.lower().split()\n",
    "    # 返回words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  sentiment                                             review\n",
      "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
      "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
      "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
      "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
      "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
      "           id                                             review\n",
      "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
      "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
      "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
      "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
      "4   \"12128_7\"  \"A very accurate depiction of small time mob l...\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "train = pd.read_csv('/Users/frank/Documents/workspace/kaggle/dataset/Bag_of_Words_Meets_Bags_of_Popcorn/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('/Users/frank/Documents/workspace/kaggle/dataset/Bag_of_Words_Meets_Bags_of_Popcorn/testData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "print train.head()\n",
    "print test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with all this stuff going down at the moment with mj i ve started listening to his music watching the odd documentary here and there watched the wiz and watched moonwalker again maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent moonwalker is part biography part feature film which i remember going to see at the cinema when it was originally released some of it has subtle messages about mj s feeling towards the press and also the obvious message of drugs are bad m kay visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him the actual feature film bit when it finally starts is only on for minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord why he wants mj dead so bad is beyond me because mj overheard his plans nah joe pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno maybe he just hates mj s music lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence also the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene bottom line this movie is for people who like mj on one level or another which i think is most people if not then stay away it does try and give off a wholesome message and ironically mj s bestest buddy in this movie is a girl michael jackson is truly one of the most talented people ever to grace this planet but is he guilty well with all the attention i ve gave this subject hmmm well i don t know because people can be different behind closed doors i know this for a fact he is either an extremely nice but stupid guy or one of the most sickest liars i hope he is not the latter \n",
      "\n",
      "naturally in a film who s main themes are of mortality nostalgia and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones however there is a craftsmanship and completeness to the film which anyone can enjoy the pace is steady and constant the characters full and engaging the relationships and interactions natural showing that you do not need floods of tears to show emotion screams to show fear shouting to show dispute or violence to show anger naturally joyce s short story lends the film a ready made structure as perfect as a polished diamond but the small changes huston makes such as the inclusion of the poem fit in neatly it is truly a masterpiece of tact subtlety and overwhelming beauty\n"
     ]
    }
   ],
   "source": [
    "# 预处理数据\n",
    "label = train['sentiment']\n",
    "train_data = []\n",
    "for i in range(len(train['review'])):\n",
    "    train_data.append(' '.join(review_to_wordlist(train['review'][i])))\n",
    "test_data = []\n",
    "for i in range(len(test['review'])):\n",
    "    test_data.append(' '.join(review_to_wordlist(test['review'][i])))\n",
    "\n",
    "# 预览数据\n",
    "print train_data[0], '\\n'\n",
    "print test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征处理\n",
    "直接丢给计算机这些词文本，计算机是无法计算的，因此我们需要把文本转换为向量，有几种常见的文本向量处理方法，比如：\n",
    "1. 单词计数  \n",
    "2. TF-IDF向量  \n",
    "3. Word2vec向量  \n",
    "我们先使用TF-IDF来试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF处理结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "tfidf = TFIDF(min_df=2, # 最小支持度为2\n",
    "           max_features=None, \n",
    "           strip_accents='unicode', \n",
    "           analyzer='word',\n",
    "           token_pattern=r'\\w{1,}', \n",
    "           ngram_range=(1, 3),  # 二元文法模型\n",
    "           use_idf=1,\n",
    "           smooth_idf=1,\n",
    "           sublinear_tf=1, \n",
    "           stop_words = 'english') # 去掉英文停用词\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "tfidf.fit(data_all)\n",
    "data_all = tfidf.transform(data_all)\n",
    "# 恢复成训练集和测试集部分\n",
    "train_x = data_all[:len_train] \n",
    "test_x = data_all[len_train:]\n",
    "print 'TF-IDF处理结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项式贝叶斯分类器10折交叉验证得分:  0.94983968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "model_NB = MNB()\n",
    "model_NB.fit(train_x, label) \n",
    "MNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print \"多项式贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "test_predicted = np.array(model_NB.predict(test_x))\n",
    "print '保存结果...'\n",
    "nb_output = pd.DataFrame(data=test_predicted, columns=['sentiment'])\n",
    "nb_output['id'] = test['id']\n",
    "nb_output = nb_output[['id', 'sentiment']]\n",
    "nb_output.to_csv('nb_output.csv', index=False)\n",
    "print '结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 提交最终的结果到kaggle，AUC为：0.85728，排名300左右，50%的水平  \n",
    "2. ngram_range = 3, 三元文法，AUC为0.85924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.96497, std: 0.00476, params: {'C': 30}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# 设定grid search的参数\n",
    "grid_values = {'C':[30]}  \n",
    "# 设定打分为roc_auc\n",
    "model_LR = GridSearchCV(LR(penalty = 'L2', dual = True, random_state = 0), grid_values, scoring = 'roc_auc', cv = 20) \n",
    "model_LR.fit(train_x, label)\n",
    "# 20折交叉验证\n",
    "GridSearchCV(cv=20, estimator=LR(C=1.0, class_weight=None, dual=True, \n",
    "             fit_intercept=True, intercept_scaling=1, penalty='L2', random_state=0, tol=0.0001),\n",
    "        fit_params={}, iid=True, n_jobs=1,\n",
    "        param_grid={'C': [30]}, pre_dispatch='2*n_jobs', refit=True,\n",
    "        scoring='roc_auc', verbose=0)\n",
    "#输出结果\n",
    "print model_LR.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "test_predicted = np.array(model_LR.predict(test_x))\n",
    "print '保存结果...'\n",
    "lr_output = pd.DataFrame(data=test_predicted, columns=['sentiment'])\n",
    "lr_output['id'] = test['id']\n",
    "lr_output = lr_output[['id', 'sentiment']]\n",
    "lr_output.to_csv('lr_output.csv', index=False)\n",
    "print '结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 提交最终的结果到kaggle，AUC为：0.88956，排名260左右，比之前贝叶斯模型有所提高   \n",
    "2. 三元文法，AUC为0.89076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "神经网络语言模型L = SUM[log(p(w|contect(w))]，即在w的上下文下计算当前词w的概率，由公式可以看到，我们的核心是计算p(w|contect(w)， Word2vec给出了构造这个概率的一个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "   \n",
    "    words = review_text.lower().split()\n",
    "   \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "  \n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    '''\n",
    "    将评论段落转换为句子，返回句子列表，每个句子由一堆词组成\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip().decode('utf8'))\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            # 获取句子中的词列表\n",
    "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Library/Python/2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Library/Python/2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Library/Python/2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Library/Python/2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Library/Python/2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理unlabeled_train data...\n",
      "25000\n",
      "795538\n"
     ]
    }
   ],
   "source": [
    "unlabeled_train = pd.read_csv(\"/Users/frank/Documents/workspace/kaggle/dataset/Bag_of_Words_Meets_Bags_of_Popcorn/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "print '预处理unlabeled_train data...'\n",
    "print len(train_data)\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "# 模型参数\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型中...\n",
      "CPU times: user 6min 16s, sys: 8.34 s, total: 6min 24s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练模型\n",
    "print(\"训练模型中...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存模型...\n"
     ]
    }
   ],
   "source": [
    "print '保存模型...'\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预览模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6246455907821655),\n",
       " (u'lady', 0.6008599400520325),\n",
       " (u'lad', 0.5698915719985962),\n",
       " (u'businessman', 0.5431989431381226),\n",
       " (u'chap', 0.53116375207901),\n",
       " (u'monk', 0.5250570774078369),\n",
       " (u'men', 0.5177899599075317),\n",
       " (u'guy', 0.517480731010437),\n",
       " (u'farmer', 0.5114585757255554),\n",
       " (u'person', 0.5109285116195679)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'princess', 0.6759523153305054),\n",
       " (u'bride', 0.6207793951034546),\n",
       " (u'belle', 0.6001157760620117),\n",
       " (u'shearer', 0.5995810031890869),\n",
       " (u'stepmother', 0.596365749835968),\n",
       " (u'victoria', 0.5917614698410034),\n",
       " (u'dame', 0.589063286781311),\n",
       " (u'latifah', 0.5790275931358337),\n",
       " (u'countess', 0.5776904821395874),\n",
       " (u'widow', 0.5727116465568542)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7642339468002319),\n",
       " (u'atrocious', 0.7405279874801636),\n",
       " (u'horrible', 0.7376815676689148),\n",
       " (u'abysmal', 0.7010303139686584),\n",
       " (u'dreadful', 0.6942194104194641),\n",
       " (u'appalling', 0.6887971758842468),\n",
       " (u'lousy', 0.6646767854690552),\n",
       " (u'horrid', 0.6554058194160461),\n",
       " (u'horrendous', 0.6533403992652893),\n",
       " (u'amateurish', 0.6079087853431702)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
