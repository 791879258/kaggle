{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 电影文本情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github地址](https://github.com/lijingpeng/kaggle/tree/master/competitions/Bag_of_Words)  \n",
    "[Kaggle地址](https://www.kaggle.com/c/word2vec-nlp-tutorial/)\n",
    "\n",
    "这个任务主要是对电影评论文本进行情感分类，主要分为正面评论和负面评论，所以是一个二分类问题，二分类模型我们可以选取一些常见的模型比如贝叶斯、逻辑回归等，这里挑战之一是文本内容的向量化，因此，我们首先尝试基于TF-IDF的向量化方法，然后尝试word2vec。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "    '''\n",
    "    # 去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 小写化所有的词，并转成词list\n",
    "    words = review_text.lower().split()\n",
    "    # 返回words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 载入数据集\n",
    "train = pd.read_csv('/Users/frank/Documents/workspace/kaggle/dataset/Bag_of_Words_Meets_Bags_of_Popcorn/labeledTrainData.tsv', header=0, delimiter=\"\\t\")\n",
    "test = pd.read_csv('/Users/frank/Documents/workspace/kaggle/dataset/Bag_of_Words_Meets_Bags_of_Popcorn/testData.tsv', header=0, delimiter=\"\\t\")\n",
    "print train.head()\n",
    "print test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "label = train['sentiment']\n",
    "train_data = []\n",
    "for i in range(len(train['review'])):\n",
    "    train_data.append(' '.join(review_to_wordlist(train['review'][i])))\n",
    "test_data = []\n",
    "for i in range(len(test['review'])):\n",
    "    test_data.append(' '.join(review_to_wordlist(test['review'][i])))\n",
    "\n",
    "# 预览数据\n",
    "print train_data[0], '\\n'\n",
    "print test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征处理\n",
    "直接丢给计算机这些词文本，计算机是无法计算的，因此我们需要把文本转换为向量，有几种常见的文本向量处理方法，比如：\n",
    "1. 单词计数  \n",
    "2. TF-IDF向量  \n",
    "3. Word2vec向量  \n",
    "我们先使用TF-IDF来试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "tfidf = TFIDF(min_df=3, # 最小支持度为3\n",
    "           max_features=None, \n",
    "           strip_accents='unicode', \n",
    "           analyzer='word',\n",
    "           token_pattern=r'\\w{1,}', \n",
    "           ngram_range=(1, 2),  # 二元文法模型\n",
    "           use_idf=1,\n",
    "           smooth_idf=1,\n",
    "           sublinear_tf=1, \n",
    "           stop_words = 'english') # 去掉英文停用词\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "tfidf.fit(data_all)\n",
    "data_all = tfidf.transform(data_all)\n",
    "# 恢复成训练集和测试集部分\n",
    "train_x = data_all[:len_train] \n",
    "test_x = data_all[len_train:]\n",
    "print 'TF-IDF处理结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "model_NB = MNB()\n",
    "model_NB.fit(train_x, label) \n",
    "MNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print \"多项式贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_predicted = np.array(model_NB.predict(test_x))\n",
    "print '保存结果...'\n",
    "nb_output['id'] = test['id']\n",
    "nb_output['sentiment'] = pd.DataFrame(data=test_predicted)\n",
    "nb_output.to_csv('nb_output.csv', index=False)\n",
    "print '结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交最终的结果到kaggle，AUC为：0.85728，排名300左右，50%的水平"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# 设定grid search的参数\n",
    "grid_values = {'C':[30]}  \n",
    "# 设定打分为roc_auc\n",
    "model_LR = GridSearchCV(LR(penalty = 'L2', dual = True, random_state = 0), grid_values, scoring = 'roc_auc', cv = 20) \n",
    "model_LR.fit(train_x, label)\n",
    "# 20折交叉验证\n",
    "GridSearchCV(cv=20, estimator=LR(C=1.0, class_weight=None, dual=True, \n",
    "             fit_intercept=True, intercept_scaling=1, penalty='L2', random_state=0, tol=0.0001),\n",
    "        fit_params={}, iid=True, n_jobs=1,\n",
    "        param_grid={'C': [30]}, pre_dispatch='2*n_jobs', refit=True,\n",
    "        scoring='roc_auc', verbose=0)\n",
    "#输出结果\n",
    "print model_LR.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predicted = np.array(model_LR.predict(test_x))\n",
    "print '保存结果...'\n",
    "lr_output = pd.DataFrame(data=test_predicted, columns=['sentiment'])\n",
    "lr_output['id'] = test['id']\n",
    "lr_output = lr_output[['id', 'sentiment']]\n",
    "lr_output.to_csv('lr_output.csv', index=False)\n",
    "print '结束.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交最终的结果到kaggle，AUC为：0.88956，排名260左右，比之前贝叶斯模型有所提高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "神经网络语言模型L = SUM[log(p(w|contect(w))]，即在w的上下文下计算当前词w的概率，由公式可以看到，我们的核心是计算p(w|contect(w)， Word2vec给出了构造这个概率的一个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(train_data, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
